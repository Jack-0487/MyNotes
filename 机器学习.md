# 机器学习

## 绪论

-  数据集：记录的集合
- 数据集中每条记录是关于一个事件或对象的描述，称为一个**示例**
- 反应事件或对象在某方面的表现或性质的事项称为**属性或特征**；属性的值为属性值；属性张成的空间为**属性空间/样本空间/输入空间**；由于空间中的每个点对应一个坐标向量，因此把一个示例称为**特征向量**
- 从数据中学的模型的过程称为**学习/训练**；训练过程中使用的数据为**训练数据**；其中每一个样本称为**训练样本**，训练样本的集合称为**训练集**；学习过程就是为了找出或逼近真相，模型也称为**学习器**；
- 关于示例结果的信息称为**标记**（如“好瓜”）；有了标记信息的示例称为**样例**；所有标记的集合称为**标记空间/输出空间**
- 预测的离散值则为**分类**（二分类和多分类）；预测的连续值则为**回归**
- 学得模型后使用其进行预测的过程称为**测试**，被预测的样本称为**测试样本**
- **聚类**：聚类学习过程中使用的训练样本通常不含有标记信息
- 根据训练数据是否有标记信息学习任务可以分为**监督学习**（分类和回归）和**无监督学习**（聚类）
- 学得模型适用于新样本的能力称为**泛化能力**
- 存在着一个与训练集一致的假设集合称之为**版本空间**
- 机器学习算法在学习过程中对某种类型假设的偏好称为**归纳偏好**；任何一个有效的机器学习算法必有其归纳偏好
- **奥卡姆剃须刀**：若有多分假设与观察一致，则选择最简单的那个

## 模型评估与选择

- 错误率E：分类错误栈样本总数的比例

- 精度：1-E

- 训练误差：在训练集中的误差

- 泛化误差：在新样本中的误差

- 过拟合：学习能力过于强大，泛化性能不足；过拟合无法避免，只能缓解/减小风险

- 欠拟合：学习能力低下，对训练样本的一般性质尚未学好

- 评估方法：

  - 需要一个测试集来测试模型对新样本的判别能力，以测试集的测试误差作为泛化误差近似

  - 测试集应该与训练集互斥（测试集不得在训练集中使用）

  - 留出法：对数据集划分为两个互斥的集合，一个为训练集，一个为测试集

    - 测试集与训练集的划分需要保持数据分布的一致性，通常采用分层采样
    - 约2/3~4/5的样本用于训练，其余样本用于测试

  - 交叉验证法：将数据集划分为k个大小相似的互斥子集，每个子集都在D中采用分层采样得到；每次用k-1个子集的并集作为训练集，剩下的子集为验证集，这样可以进行k次训练，最终返回k个测试结果的均值

    - k通常为10
    - 称为**k折交叉验证**
    - 留一法

  - 自助法：对于包含m个样本的数据集D，每次从中选出一个样本，将其拷贝放入采样产生的数据集D'中，执行m次采样，得到含有m个样本的数据集D’，D作为训练集，D/D'作为测试集

    - 某个样本在m次采样中始终为被采样的概率为
      $$
      (1-1/m)^m = 1/e = 0.368
      $$

      - 自助法在数据集较小，难以有效划分训练/测试集时很有用
      - 自助法会引入估计偏差

  - 模型评估与选择中用于评估测试的数据集为验证集

- 性能度量：衡量模型泛化能力，反应任务需求

  - 错误率与精度
  - 查全率和查准率
    - 查全率和查准率是矛盾的度量
    - 以查准率为纵轴，查全率为横轴作图得到P-R曲线（P-R图）
  -  ROC曲线：纵轴是真正例率（TPR），横轴是假正例率（FPR）
    - 两个模型的ROC曲线相交，一般比较ROC曲线下的面积（AUC）来比较两个算法的优劣
  - 代价敏感错误率与代价曲线

- 比较检验：

  - 泛化错误率为ε的模型在一个样本上犯错的概率是ε，测试错误率ω意味着在M个测试样本中恰有ω*M个样本被错误分类

- **偏差**度量了学习算法的期望预测与真实结果的偏离程度，即刻画了算法本身的拟合能力；**方差**度量了同样大小的训练集的变动所导致的学习性能的变化，刻画了数据扰动所造成的影响；**噪声**表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度

- 泛化性能由算法能力，数据的充分性以及学习任务本身的难度所共同决定的

- 偏差-方差窘境

## 监督学习(supervised learning)

- 监督学习:数据集有标注，“知道正确答案”
- 回归问题（regression）：输出为连续的值
- 分类问题：预测离散输出，输出的离散值可以有多个
- 无穷多个特征：支持向量机

## 无监督学习（Unsupervised l- earning）

- 聚类算法：将数据分为几个簇
- 数据集无标注
- 鸡尾酒会算法
- octave编程环境

## 模型描述

- m表示训练样本数量
- x表述输入变量（特征）
- y表示输出变量
- (x,y)表示一个训练样本；(x<sup>(i)</sup>,y<sup>(i)</sup>)表示第i个样本
- 假设函数h：引导x输出y的函数
- 一元线性回归（单变量线性回归）：y=θ<sub>0</sub>+θ<sub>1</sub>x

## 代价函数

- 模型参数：θ<sub>0</sub>和θ<sub>1</sub>

- 最小二乘法

- 代价函数(cost function)
  $$
  J(θ_0,θ_1)= \frac{1}{2m}\sum(h_\theta x^{(i)}-y^{(i)})^2
  $$
  
  $$
  minimize  J(\theta_1,\theta_2)
  $$

## 梯度下降法

- 步骤

  - 给定θ<sub>0</sub>和θ<sub>1</sub>的初值；通常都为0
  - 不断改变θ<sub>0</sub>和θ<sub>1</sub>的值使得代价函数J取得最小值（或者局部最小值）

- 起始点不一样会导致梯度下降法的局部最优解不一样

- 学习率α

- $$
  \theta_j = \theta_j - \alpha \frac{\partial{(\theta_0,\theta_1)}}{\partial{\theta_j}}
  $$

- 同步更新

- 在接近最优解时，导数值会越来越小，接近的步伐会越来越小

- $$
  \theta_0 = \frac{1}{m}\sum{(h_\theta(x^{(i)})-y^{(i)})}
  $$

- $$
  \theta_1 = \frac{1}{m}\sum{(h_\theta(x^{(i)})-y^{(i)})}*x^{(i)}
  $$

- Batch 

## 多元变量

- 多个特征值n

- x<sup>(i)</sup>表示第i个训练样本

- x<sub>j</sub><sup>(i)</sup>表示第i个样本中的第j个特征值

- 此时假设函数
  $$
  h_\theta(x)=\theta_0+\theta_1x_1+\theta_2x_2+....+\theta_nx_n=\theta^Tx
  $$
  默认x<sub>0</sub>=1

- 多元线性回归梯度下降法：
  $$
  \theta_j = \theta_j-\alpha\frac{1}{m}\sum{(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}}
  $$

## 特征缩放（归一化）

- 通常缩放到[-1,1]\[-3,3]

- 均值标准化
  $$
  x_1 = \frac{x_1-u_1}{s_1}
  $$
  u<sub>1</sub>为训练样本x<sub>1</sub>的特征值的平均值,s<sub>1</sub>为特征值的取值范围

- 学习率足够小，梯度下降法收敛

## 特征和多项式回归

## 正规方程

- 最小二乘法

- 设计矩阵X（m*（n+1））

- 代价函数θ
  $$
  \theta = (X^TX)^{-1}X^Ty
  $$

- 正规方程法不需要特征缩放
- 正规方程法和梯度下降法：
  - 正规方程法不需要选择学习率α，不需要迭代
  - 梯度下降法需要选择学习率α，需要进行多次迭代
  - 特征较多（>10000）时正规方程计算较慢（需要计算矩阵的逆）

## 分类/逻辑回归算法

- 负类/正类（0/1）

- 多分类问题

- 阈值（threshold）

- 逻辑回归（logistic regression）为分类算法

- 假设函数h<sub>θ</sub>(x)

- Sigmoid函数(or logistic function)：
  $$
  g(z) = \frac{1}{1+e^{-z}}
  $$

$$
h_\theta(x)=\theta^Tx
$$

$$
g(\theta^Tx) = \frac{1}{1+e^{-\theta^Tx}}
$$

- g(z):z<0,g(z) < 0.5 and g(z) > 0; z>0, 0.5 < g(z) < 1
- h(θ) = P(y = 1 | x; θ)：在给定x的条件下y = 1的概率

### 决策边界

## 代价函数

- 如何选择拟合参数θ

  - 线性回归
    $$
    J(\theta) = \frac{1}{m}\sum\frac{1}{2}(h_\theta(x^{(i)})-y^{(i)})^2 = \frac{1}{m}\sum cost(h_\theta(x^{(i)},y^{(i)})
    $$

    $$
    cost(h_\theta(x),y)=\frac{1}{2}(h_\theta(x)-y)^2
    $$

  - 逻辑回归
    $$
    cost(h_\theta(x),y) = \begin{cases} 
    -log(h_\theta(x))& y = 1\\ -log(1-h_\theta(x))& y = 0
    \end{cases}
    $$
    预测值与实际值相差越大，代价函数越大

    - 优化逻辑回归代价函数
      $$
      cost(h_\theta(x),y) = -ylog(h_\theta(x))-(1-y)log(1-h_\theta(x))
      $$

    - 高级优化
      - 共轭梯度法
      - （L-BFGS）
      - BFGS
      - 优点：收敛速度快；缺点：复杂

## 多元分类

$$
h_\theta^{(i)}(x)=P(y=i|x;\theta) i = 1, 2, 3, 4...
$$

## 过拟合

- 很好的拟合训练集的曲线，代价函数接近于0，泛化能力不足，具有高方差
- 解决过拟合
  - 减少特征数量
  - 正则化，减少量级或缩小x的取值范围

## 正则化

- θ<sub>1</sub>,θ<sub>2</sub>,θ<sub>3</sub>,θ<sub>4</sub>为参数，若θ<sub>3</sub>,θ<sub>4</sub>非常小，通过加惩罚增大这两个参数
  $$
  \underset {\theta}{min}\frac{1}{2m}\sum{(h_\theta(x^{(i)})-y^{(i)})^2}+1000\theta_3^2+1000\theta_4^2
  $$

- 正则化：使得模型的参数尽可能的小

- θ<sub>0</sub>一般不加惩罚项

- $$
  J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{i})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]
  $$

  其中λ为正则化参数，λ的作用是控制两个不同目标之间的取舍（第一个目标是使模型尽可能的拟合训练集，第二个目标是使模型的参数尽可能的小），从而使得模型简化，避免过拟合

 ## 线性回归的正则化

$$
\theta_j=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$

$$
\theta_j=\theta_j(1-\alpha\frac{\lambda}{m})-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{(i)})-y^{(i)})x_j^{i}]其中(1-\alpha\frac{\lambda}{m})约为0.99
$$

## 逻辑回归的正则化

$$
h_\theta(x)=\frac{1}{1+e^{-\theta^Tx}}
$$

$$
\theta_j=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^{m}(h_\theta(x^{i})-y^{(i)})x_j^{(i)}+\frac{\lambda}{m}\theta_j]
$$

## 神经网络

### 非线性假设

### 神经网络模型

- x<sub>0</sub>为偏置神经元/偏置单元
- 激活函数g(z)
- θ为神经网络的参数/权重
- 神经网络是多个神经元连接在一起的集合
- 第一层为输入层，最后一层为输出层，中间层为隐藏层

![Screenshot_20211212_165512_tv.danmaku.bili](C:\Users\Dats\Desktop\矩阵论\MobileFile\Screenshot_20211212_165512_tv.danmaku.bili.jpg)



### 多元分类

### 代价函数

- L为神经网络的层数

- S<sub>l</sub>为第L层的神经元数

- $$
  J(\theta)=-\frac{1}{m}[\sum_{i=1}^m\sum_{k=1}^{K}y_k^{(i)}log(h_\theta(x^{(i)}))_k+(1-y_k^{(i)})log(1-(h_\theta(x^{(i)}))_k)]+\frac{\lambda}{2m}\sum_{l=1}^{L-1}\sum_{i=1}^{s_l}\sum_{j=1}^{s_{l+1}}(\theta_{ji}^{l})^2
  $$

- 其中(h<sub>θ</sub>(x))<sub>i</sub>为第i层输出

### 反向传播算法

$$
\delta_j^{(i)}=a_j^{(i)}-y_j\ (i >=1)
$$

- δ为第j层第i个结点的误差i（假设输出减去真实输出）

- $$
  \delta^{(3)}=(\theta^{3})^T\delta^{(4)}.*g'(z^{(3)})
  $$

- $$
  \delta_j^{(l)}=\frac{\partial}{\partial{z_j^{l}}}cost(i)\\cost(i)=y^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})logh_\theta(x^{(i)})
  $$

- ![47465316398839092](C:\Users\Dats\Desktop\笔记\47465316398839092.png)

 ### 梯度检验

$$
\frac{\partial{J(\theta)}}{\partial(\theta)}=\frac{J(\theta+\epsilon)-J(\theta-\epsilon)}{2\epsilon}\\\epsilon=10^-4
$$

- 对于向量θ
  $$
  \theta=[\theta_1,\theta_2,\theta_3...,\theta_n]\\
  \frac{\partial(J(\theta))}{\partial\theta_n}=\frac{J(\theta_1,\theta_2,...,\theta_n+\epsilon)-J(\theta_1,\theta_2,...,\theta_n-\epsilon)}{2\epsilon}
  $$
  

### 随机初始化

- θ的范围为[-ε，ε]

### 总结

- 隐藏层通常越多越好
- 如何选择神经网络的结构
- 训练神经网络的步骤
  - 随机初始化权重
  - 利用前向传播算法计算h<sub>θ</sub>(x<sup>(i)</sup>)
  - 计算代价函数J(θ)
  - 利用反向传播算法计算J(θ)的偏导数

## 模型评估

### 评估假设

- 训练集：测试集=7: 3

### 模型选择

- 训练集，测试集，验证集=6: 2: 2

- 交叉验证集（验证集）cv

- $$
  training\;error:J_{train}(\theta)=\frac{1}{2m}\sum_{i=1}^m(h_\theta(x)^{(i)}-y^{(i)})^2\\
  cross\;Validation{\quad}error:J_{cv}(\theta)=\frac{1}{2m_{cv}}\sum_{i=1}^{m_{cv}}(h_\theta(x_{cv}^{(i)})-y_{cv}^{(i)})^2\\
  test\;error:J_{test}(\theta)=\frac{1}{2m_{test}}\sum_{i=1}^{m_{test}}(h_\theta(x_{test}^{(i)})-y_{test}^{(i)})^2
  $$

### 诊断偏差与方差

- 偏差和方差大-欠拟合；偏差小方差大-过拟合

### 正则化，偏差与方差

- $$
  J(\theta)=\frac{1}{2m}[\sum_{i=1}^m(h_\theta(x^{i})-y^{(i)})^2+\lambda\sum_{j=1}^n\theta_j^2]\\
  h_\theta(x)=\theta_0+\theta_1x+\theta_2x^2+\theta_3x^3+\theta_4x^4
  $$

- λ过大时，欠拟合，λ过小时过拟合

### 学习曲线

- 高偏差将导致J<sub>cv</sub>和J<sub>train</sub>较大
- 当模型具有高偏差时，即使增加样本也不能降低验证集和测试集的误差
- 当模型具有高方差时，交叉验证误差和训练偏差相差较大，训练集样本增加时，验证集误差会减小
- 高方差/过拟合原因：模型过于复杂，训练样本太少
- 高偏差：欠拟合
- ![68687816405129962](C:\Users\Dats\Desktop\笔记\68687816405129962.png)

## 机器学习系统设计

- 误差分析
- 过早优化问题
- 评价指标
- 先建立一个简单的算法模型，通过指标去观察误差大小，根据误差去改进模型

### 误差评估

- 查准率/查全率:对于正例较少的情况，查准率和查全率可以较好的评估算法

  - | 真实情况 |   预测结果   |   预测结果   |
    | :------: | :----------: | :----------: |
    |          |     正例     |     反例     |
    |   正例   | TP（真正例） | FN（假反例） |
    |   反例   | FP（假正例） | TN（真反例） |

  - 查准率
    $$
    P=\frac{TP}{TP+FP}
    $$

  - 查全率
    $$
    R=\frac{TP}{TP+FN}
    $$

- 调和平均数(利用调和平均数来选择算法)（选择F大的算法）
  $$
  F = 2\frac{P*R}{p+R}
  $$

## 机器学习数据

- 如果有一个非常大的训练集，则模型几乎不会过拟合，测试误差将接近于训练误差

## 支持向量机

- ![47234316406783082](C:\Users\Dats\Desktop\笔记\47234316406783082.png)  

- $$
  \min_\theta C\sum_{i=1}^{m}[y^{(i)}cost_1(\theta^Tx^{(i)})+(1-y^{(i)})cost_0(\theta^Tx^{(i)})]+\frac{1}{2}\sum_{i=1}^n\theta_j^2\\cost_1(\theta^Tx^{(i)})=-logh_\theta(x^{(i)})\\cost_0(\theta^Tx^{(i)})=-log(1-h_\theta(x^{(i)}))\\h_\theta(x)=\begin{cases}1&\theta^Tx>=0\\0&otherwise\end{cases}\\C通常为极大值，\frac{1}{λ}
  $$

- 支持向量机也称大间距分类器

  ![](C:\Users\Dats\Desktop\笔记\bili_poster-1645430235218.png)

- 支持向量机的间距

- 对异常点敏感

### SVM数学原理

- 向量内积就是投影长度
- 决策边界![bili_poster-1645432063449](C:\Users\Dats\Desktop\笔记\bili_poster-1645432063449.png)

### 核函数

- 高斯核函数
- 线性核函数
- 默塞尔定理
- 多项式核函数（效果较差，不使用）/卡方核函数/字符串核函数

### 使用SVM

SVM software package: liblinear/libsvm![Screenshot_20220222_161535_tv.danmaku.bili](C:\Users\Dats\Desktop\笔记\Screenshot_20220222_161535_tv.danmaku.bili.jpg)

## 无监督学习

### 聚类算法(Clustering)

- 用途：市场分割/社交网络分析/组织计算机集群，布局网络/

#### K均值算法

- 聚类中心

- K均值算法

  - $$
    K-聚类中心数\\无标签训练集{x^{(1)},x^{(2)},...x^{(m)}}\\
    $$

  - K均值算法的优化目标函数![Screenshot_20220222_170429_tv.danmaku.bili](C:\Users\Dats\Desktop\笔记\Screenshot_20220222_170429_tv.danmaku.bili.jpg)

- ![Screenshot_20220222_170614_tv.danmaku.bili](C:\Users\Dats\Desktop\笔记\Screenshot_20220222_170614_tv.danmaku.bili.jpg)

- 多次随机初始化得到全局最优![Screenshot_20220223_143350_tv.danmaku.bili](C:\Users\Dats\Desktop\笔记\Screenshot_20220223_143350_tv.danmaku.bili.jpg)

- 聚类数量和参数K的选取-观察输出

## 数据压缩

- 将二维数据变成一位数据

## 可视化

## 主成成分分析（PCA）

- 作用：数据压缩

- 投影误差
- PCA与线性回归区别![Screenshot_20220223_153456_tv.danmaku.bili](C:\Users\Dats\Desktop\笔记\Screenshot_20220223_153456_tv.danmaku.bili.jpg)







![Screenshot_20220223_160227_tv.danmaku.bili](C:\Users\Dats\Desktop\笔记\Screenshot_20220223_160227_tv.danmaku.bili.jpg)

## 高斯分布

